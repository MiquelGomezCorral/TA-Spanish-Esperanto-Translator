{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f8669b",
   "metadata": {},
   "source": [
    "# PATH CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785d53e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/turbotowerlnx/Documents/Master/TA/TA-Spanish-Esperanto-Translator/notebooks\n",
      "/home/turbotowerlnx/Documents/Master/TA/TA-Spanish-Esperanto-Translator/app\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "if not os.getcwd().endswith(\"app\"):\n",
    "    os.chdir(\"../app\")\n",
    "    print(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9f544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import Configuration\n",
    "\n",
    "CONFIG = Configuration(\n",
    "    model_name=\"facebook/nllb-200-distilled-600M\",\n",
    "    src_code = \"spa_Latn\",\n",
    "    tgt_code = \"epo_Latn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abbd27",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48be33bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "  Train: 4019270 (70.0%)\n",
      "  Val:   861272 (15.0%)\n",
      "  Test:  861272 (15.0%)\n",
      "  Total: 5741814\n"
     ]
    }
   ],
   "source": [
    "df_corpus_clean = pd.read_csv(CONFIG.corpus_path)\n",
    "df_corpus_clean.rename(columns={\n",
    "    CONFIG.src_name: CONFIG.src_code, \n",
    "    CONFIG.tgt_name: CONFIG.tgt_code\n",
    "}, inplace=True)\n",
    "\n",
    "# Shuffle the dataframe first\n",
    "df_shuffled = df_corpus_clean.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Calculate split indices\n",
    "n_total = len(df_shuffled)\n",
    "n_test = int(n_total * CONFIG.test_split)\n",
    "n_val = int(n_total * CONFIG.val_split)\n",
    "\n",
    "# Split the data\n",
    "df_test = df_shuffled[:n_test].reset_index(drop=True)\n",
    "df_val = df_shuffled[n_test:n_test + n_val].reset_index(drop=True)\n",
    "df_train = df_shuffled[n_test + n_val:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(df_train)} ({len(df_train)/n_total*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(df_val)} ({len(df_val)/n_total*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(df_test)} ({len(df_test)/n_total*100:.1f}%)\")\n",
    "print(f\"  Total: {n_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70579d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/turbotowerlnx/Documents/Master/TA/TA-Spanish-Esperanto-Translator/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.data import TranslationDataset\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG.model_name, \n",
    "    padding=True, \n",
    "    pad_to_multiple_of=8, \n",
    "    src_lang=CONFIG.src_code, \n",
    "    tgt_lang=CONFIG.tgt_code, \n",
    "    truncation=True, \n",
    "    max_length=CONFIG.max_tok_length,\n",
    ")\n",
    "\n",
    "dataloader_train = TranslationDataset(\n",
    "    df_train,\n",
    "    tokenizer,\n",
    "    CONFIG.src_code,\n",
    "    CONFIG.tgt_code,\n",
    "    CONFIG.max_tok_length,\n",
    ")\n",
    "dataloader_val = TranslationDataset(\n",
    "    df_val,\n",
    "    tokenizer,\n",
    "    CONFIG.src_code,\n",
    "    CONFIG.tgt_code,\n",
    "    CONFIG.max_tok_length,\n",
    ")\n",
    "dataloader_test = TranslationDataset(\n",
    "    df_test,\n",
    "    tokenizer,\n",
    "    CONFIG.src_code,\n",
    "    CONFIG.tgt_code,\n",
    "    CONFIG.max_tok_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f80ed",
   "metadata": {},
   "source": [
    "# Load transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc98e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    CONFIG.model_name,\n",
    "    quantization_config=quantization_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b6634",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20aad251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/turbotowerlnx/Documents/Master/TA/TA-Spanish-Esperanto-Translator/venv/lib/python3.12/site-packages/torchmetrics/utilities/imports.py:23: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 96642.95it/s]\n",
      "\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.6. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.6. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "Encoder model frozen.\n",
      "/home/turbotowerlnx/Documents/Master/TA/TA-Spanish-Esperanto-Translator/venv/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "/home/turbotowerlnx/Documents/Master/TA/TA-Spanish-Esperanto-Translator/venv/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "metric_bleu = load(\"sacrebleu\")\n",
    "metric_comet = load(\"comet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da1d81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels, sources = eval_preds\n",
    "\n",
    "    # Convert to lists if coming from a datasets.Column\n",
    "    if not isinstance(labels, list):\n",
    "        labels = list(labels)\n",
    "        \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace negative ids in the labels as we can't decode them.\n",
    "    labels = [\n",
    "        [tokenizer.pad_token_id if j < 0 else j for j in label]\n",
    "        for label in labels\n",
    "    ]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Decode sources\n",
    "    decoded_sources = tokenizer.batch_decode(sources, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result_blue = metric_bleu.compute(\n",
    "        predictions=decoded_preds, \n",
    "        references=decoded_labels\n",
    "    )\n",
    "    result_comet = metric_comet.compute(\n",
    "        sources=decoded_sources,\n",
    "        predictions=decoded_preds, \n",
    "        references=[label[0] for label in decoded_labels]  # COMET expects flat list, not nested\n",
    "    )\n",
    "    result = {\n",
    "        \"bleu\": result_blue[\"score\"],\n",
    "        \"comet\": result_comet[\"mean_score\"]\n",
    "    }\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b5c8e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2769ae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(\n",
    "    CONFIG.model_name,\n",
    ")\n",
    "\n",
    "print(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0633a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_batch_size = 32\n",
    "test_loader = DataLoader(dataloader_test, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2223b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/turbotowerlnx/Documents/Master/TA/TA-Spanish-Esperanto-Translator/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/26915 batches\n",
      "Processed 20/26915 batches\n",
      "Processed 20/26915 batches\n",
      "Processed 30/26915 batches\n",
      "Processed 30/26915 batches\n",
      "Processed 40/26915 batches\n",
      "Processed 40/26915 batches\n",
      "Processed 50/26915 batches\n",
      "Processed 50/26915 batches\n",
      "Processed 60/26915 batches\n",
      "Processed 60/26915 batches\n",
      "Processed 70/26915 batches\n",
      "Processed 70/26915 batches\n",
      "Processed 80/26915 batches\n",
      "Processed 80/26915 batches\n",
      "Processed 90/26915 batches\n",
      "Processed 90/26915 batches\n",
      "Processed 100/26915 batches\n",
      "Processed 100/26915 batches\n",
      "Processed 110/26915 batches\n",
      "Processed 110/26915 batches\n",
      "Processed 120/26915 batches\n",
      "Processed 120/26915 batches\n",
      "Processed 130/26915 batches\n",
      "Processed 130/26915 batches\n",
      "Processed 140/26915 batches\n",
      "Processed 140/26915 batches\n",
      "Processed 150/26915 batches\n",
      "Processed 150/26915 batches\n",
      "Processed 160/26915 batches\n",
      "Processed 160/26915 batches\n",
      "Processed 170/26915 batches\n",
      "Processed 170/26915 batches\n",
      "Processed 180/26915 batches\n",
      "Processed 180/26915 batches\n",
      "Processed 190/26915 batches\n",
      "Processed 190/26915 batches\n",
      "Processed 200/26915 batches\n",
      "Processed 200/26915 batches\n",
      "Processed 210/26915 batches\n",
      "Processed 210/26915 batches\n",
      "Processed 220/26915 batches\n",
      "Processed 220/26915 batches\n",
      "Processed 230/26915 batches\n",
      "Processed 230/26915 batches\n",
      "Processed 240/26915 batches\n",
      "Processed 240/26915 batches\n",
      "Processed 250/26915 batches\n",
      "Processed 250/26915 batches\n",
      "Processed 260/26915 batches\n",
      "Processed 260/26915 batches\n",
      "Processed 270/26915 batches\n",
      "Processed 270/26915 batches\n",
      "Processed 280/26915 batches\n",
      "Processed 280/26915 batches\n",
      "Processed 290/26915 batches\n",
      "Processed 290/26915 batches\n",
      "Processed 300/26915 batches\n",
      "Processed 300/26915 batches\n",
      "Processed 310/26915 batches\n",
      "Processed 310/26915 batches\n",
      "Processed 320/26915 batches\n",
      "Processed 320/26915 batches\n",
      "Processed 330/26915 batches\n",
      "Processed 330/26915 batches\n",
      "Processed 340/26915 batches\n",
      "Processed 340/26915 batches\n",
      "Processed 350/26915 batches\n",
      "Processed 350/26915 batches\n",
      "Processed 360/26915 batches\n",
      "Processed 360/26915 batches\n",
      "Processed 370/26915 batches\n",
      "Processed 370/26915 batches\n",
      "Processed 380/26915 batches\n",
      "Processed 380/26915 batches\n",
      "Processed 390/26915 batches\n",
      "Processed 390/26915 batches\n",
      "Processed 400/26915 batches\n",
      "Processed 400/26915 batches\n",
      "Processed 410/26915 batches\n",
      "Processed 410/26915 batches\n",
      "Processed 420/26915 batches\n",
      "Processed 420/26915 batches\n",
      "Processed 430/26915 batches\n",
      "Processed 430/26915 batches\n",
      "Processed 440/26915 batches\n",
      "Processed 440/26915 batches\n",
      "Processed 450/26915 batches\n",
      "Processed 450/26915 batches\n",
      "Processed 460/26915 batches\n",
      "Processed 460/26915 batches\n",
      "Processed 470/26915 batches\n",
      "Processed 470/26915 batches\n",
      "Processed 480/26915 batches\n",
      "Processed 480/26915 batches\n",
      "Processed 490/26915 batches\n",
      "Processed 490/26915 batches\n",
      "Processed 500/26915 batches\n",
      "Processed 500/26915 batches\n",
      "Processed 510/26915 batches\n",
      "Processed 510/26915 batches\n",
      "Processed 520/26915 batches\n",
      "Processed 520/26915 batches\n",
      "Processed 530/26915 batches\n",
      "Processed 530/26915 batches\n",
      "Processed 540/26915 batches\n",
      "Processed 540/26915 batches\n",
      "Processed 550/26915 batches\n",
      "Processed 550/26915 batches\n",
      "Processed 560/26915 batches\n",
      "Processed 560/26915 batches\n",
      "Processed 570/26915 batches\n",
      "Processed 570/26915 batches\n",
      "Processed 580/26915 batches\n",
      "Processed 580/26915 batches\n",
      "Processed 590/26915 batches\n",
      "Processed 590/26915 batches\n",
      "Processed 600/26915 batches\n",
      "Processed 600/26915 batches\n",
      "Processed 610/26915 batches\n",
      "Processed 610/26915 batches\n",
      "Processed 620/26915 batches\n",
      "Processed 620/26915 batches\n",
      "Processed 630/26915 batches\n",
      "Processed 630/26915 batches\n",
      "Processed 640/26915 batches\n",
      "Processed 640/26915 batches\n",
      "Processed 650/26915 batches\n",
      "Processed 650/26915 batches\n",
      "Processed 660/26915 batches\n",
      "Processed 660/26915 batches\n",
      "Processed 670/26915 batches\n",
      "Processed 670/26915 batches\n",
      "Processed 680/26915 batches\n",
      "Processed 680/26915 batches\n",
      "Processed 690/26915 batches\n",
      "Processed 690/26915 batches\n",
      "Processed 700/26915 batches\n",
      "Processed 700/26915 batches\n",
      "Processed 710/26915 batches\n",
      "Processed 710/26915 batches\n",
      "Processed 720/26915 batches\n",
      "Processed 720/26915 batches\n",
      "Processed 730/26915 batches\n",
      "Processed 730/26915 batches\n",
      "Processed 740/26915 batches\n",
      "Processed 740/26915 batches\n",
      "Processed 750/26915 batches\n",
      "Processed 750/26915 batches\n",
      "Processed 760/26915 batches\n",
      "Processed 760/26915 batches\n",
      "Processed 770/26915 batches\n",
      "Processed 770/26915 batches\n",
      "Processed 780/26915 batches\n",
      "Processed 780/26915 batches\n",
      "Processed 790/26915 batches\n",
      "Processed 790/26915 batches\n",
      "Processed 800/26915 batches\n",
      "Processed 800/26915 batches\n",
      "Processed 810/26915 batches\n",
      "Processed 810/26915 batches\n",
      "Processed 820/26915 batches\n",
      "Processed 820/26915 batches\n",
      "Processed 830/26915 batches\n",
      "Processed 830/26915 batches\n",
      "Processed 840/26915 batches\n",
      "Processed 840/26915 batches\n",
      "Processed 850/26915 batches\n",
      "Processed 850/26915 batches\n",
      "Processed 860/26915 batches\n",
      "Processed 860/26915 batches\n",
      "Processed 870/26915 batches\n",
      "Processed 870/26915 batches\n",
      "Processed 880/26915 batches\n",
      "Processed 880/26915 batches\n",
      "Processed 890/26915 batches\n",
      "Processed 890/26915 batches\n",
      "Processed 900/26915 batches\n",
      "Processed 900/26915 batches\n",
      "Processed 910/26915 batches\n",
      "Processed 910/26915 batches\n",
      "Processed 920/26915 batches\n",
      "Processed 920/26915 batches\n",
      "Processed 930/26915 batches\n",
      "Processed 930/26915 batches\n",
      "Processed 940/26915 batches\n",
      "Processed 940/26915 batches\n",
      "Processed 950/26915 batches\n",
      "Processed 950/26915 batches\n",
      "Processed 960/26915 batches\n",
      "Processed 960/26915 batches\n",
      "Processed 970/26915 batches\n",
      "Processed 970/26915 batches\n",
      "Processed 980/26915 batches\n",
      "Processed 980/26915 batches\n",
      "Processed 990/26915 batches\n",
      "Processed 990/26915 batches\n",
      "Processed 1000/26915 batches\n",
      "Processed 1000/26915 batches\n"
     ]
    }
   ],
   "source": [
    "output_sequences = []\n",
    "all_labels = []\n",
    "all_sources = []\n",
    "\n",
    "for i, batch in enumerate(test_loader):\n",
    "    # Store source input_ids for later decoding\n",
    "    all_sources.extend(batch['input_ids'].cpu())\n",
    "    \n",
    "    # Generate translations\n",
    "    with torch.no_grad():    \n",
    "        output_batch = model.generate(\n",
    "            generation_config=generation_config, \n",
    "            input_ids=batch['input_ids'].cuda(), \n",
    "            attention_mask=batch['attention_mask'].cuda(), \n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(CONFIG.tgt_code), \n",
    "            max_length=CONFIG.max_tok_length, \n",
    "            num_beams=1, \n",
    "            do_sample=False,\n",
    "        )\n",
    "    output_sequences.extend(output_batch.cpu())\n",
    "    all_labels.extend(batch['labels'].cpu())\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(test_loader)} batches\")\n",
    "    if i >= CONFIG.max_batches:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "180f041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/home/turbotowerlnx/Documents/Master/TA/TA-Spanish-Esperanto-Translator/venv/lib/python3.12/site-packages/torch/__init__.py:1551: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return _C._get_float32_matmul_precision()\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 5070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/home/turbotowerlnx/Documents/Master/TA/TA-Spanish-Esperanto-Translator/venv/lib/python3.12/site-packages/torch/__init__.py:1551: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return _C._get_float32_matmul_precision()\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 5070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 19.4891\n",
      "COMET score: 0.7691\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics\n",
    "result = compute_metrics((output_sequences, all_labels, all_sources))\n",
    "print(f'BLEU score: {result[\"bleu\"]}')\n",
    "print(f'COMET score: {result[\"comet\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
